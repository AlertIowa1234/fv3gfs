# This file configures the workflow to run on the xJet partition of Jet

platform: !Platform
  # Evaluate: this must be "false" to ensure disk space availability logic
  # is not run unless this file is for the current platform.
  Evaluate: false

  # name: the name of this platform; this must match what the underlying
  # scripts expect.  Note that the name does not specify the jet partition.
  name: JET

  # detect: this is a function that returns true iff the user is on GAEA
  # and false otherwise
  detect: !calc tools.isdir("/lfs3")

  # public_release_ics: location of input conditions that have been 
  # prepared for the public release.
  public_release_ics: /lfs3/projects/hfv3gfs/Samuel.Trahan/FV3GFS_V1_RELEASE/ICs

  # CHGRP_RSTPROD_COMMAND - this specifies the command to use to
  # restrict access to NOAA "rstprod" data restriction class.
  # This only used for observation processing, data assimilation, and
  # data assimilation archiving, which are not in the public release.
  CHGRP_RSTPROD_COMMAND: "chgrp rstprod"

  # NWPROD - location of the NCEP operational "nwprod" directory, which
  # only has meaning on the NCEP WCOSS machines.  It is used to get
  # the paths to certain programs and scripts.
  NWPROD: "/dev/null/global/save/glopara/nwpara"

  # DMPDIR - location of the global dump data.  This is used by the observation
  # processing scripts, which are not included in the public release.
  DMPDIR: "/dev/null/global/noscrub/dump"

  # RTMFIX - location of the CRTM fixed data files used by the GSI data
  # assimilation.  The data assimilation is not included in this public release
  # so this path is unused.
  RTMFIX: "/dev/null/da/save/Michael.Lueken/nwprod/lib/crtm/2.2.3/fix_update"

  # BASE_SVN - a directory maintained by emc global model developers
  # that contains recent versions of source code and executables for
  # various repositories.  This is used on some platforms to find
  # executables for this workflow.
  BASE_SVN: "/dev/null/global/save/glopara/svn"

  # config_base_extras - Additional configuration data to put in the
  # config.base file
  config_base_extras: |
    export POSTGRB2TBL=$PARMgfs/g2tmpl-1.5.0/params_grib2_tbl_new
    export WGRIB2=$EXECgfs/wgrib2

  # shared_accounting_ref - accounting settings for shared jobs
  shared_accounting_ref:
    queue: !calc metasched.varref('QUEUESHARED')
    project: !calc metasched.varref('CPU_PROJECT')
    partition: !calc metasched.varref('PARTITION')

  # service_accounting_ref - accounting settings for service jobs (jobs
  # that require tape or network access)
  service_accounting_ref:
    queue: !calc metasched.varref('QUEUESERV')
    project: !calc metasched.varref('CPU_PROJECT')
    partition: service

  # exclusive_accounting_ref - accounting settings for jobs that require
  # exclusive access to a node.
  exclusive_accounting_ref:
    queue: !calc metasched.varref('QUEUE')
    project: !calc metasched.varref('CPU_PROJECT')
    partition: !calc metasched.varref('PARTITION')

  # Additional variables to send to Rocoto XML entities or ecflow edits.
  metasched_more: !expand |
    {metasched.defvar("QUEUE", doc.platform.exclusive_queue)}
    {metasched.defvar("QUEUESHARED", doc.platform.shared_queue)}
    {metasched.defvar("QUEUESERV", doc.platform.service_queue)}
    {metasched.defvar("CPU_PROJECT", doc.accounting.cpu_project)}
    {metasched.defvar("PARTITION", "xjet")}

  # Queues to use for each job type
  shared_queue: batch
  service_queue: service
  exclusive_queue: batch

  # Details about the scheduler on this cluster.
  scheduler_settings: &scheduler_settings
    name: MoabTorque
    physical_cores_per_node: 24
    logical_cpus_per_core: 2
    hyperthreading_allowed: true
    indent_text: "  "

  # How to generate aprun, mpirun, srun, etc. commands.  This
  # is unused but required.
  parallelism_settings: &parallelism_settings
    <<: *scheduler_settings
    name: HydraIMPI

  # Details about tbe compute nodes in this cluster.   
  node_type_settings: &node_type_settings
    <<: *scheduler_settings
    node_type: generic

  # Generate the actual Python objects for the scheduler, parallelism,
  # and nodes:
  scheduler: !calc |
    tools.get_scheduler(scheduler_settings.name, scheduler_settings)
  parallelism: !calc |
    tools.get_parallelism(parallelism_settings.name, parallelism_settings)
  nodes: !calc |
    tools.node_tool_for(node_type_settings.node_type, node_type_settings)

  # long_term_temp - area for storage of data that must be passed
  # between jobs or shared with programs external to this workflow.
  long_term_temp: !expand "{doc.user_places.PROJECT_DIR}/{tools.env('USER')}/scrub"

  # short_term_temp - area for data that is only needed within one job:
  short_term_temp: !calc long_term_temp

  # EXP_PARENT_DIR - Parent directory  of the expdir (experiment directory)
  EXP_PARENT_DIR: !expand "{doc.user_places.PROJECT_DIR}/{tools.env('USER')}/noscrub"

  # NOSCRUB_DIR - directory for data that needs to be retained for an
  # indefinite amount of time after the simulation ends.
  NOSCRUB_DIR: !expand "{doc.places.PROJECT_DIR}/{tools.env('USER')}/noscrub"

  # SAVE_DIR - directory for small, important, files that should be
  # backed up to tape.
  SAVE_DIR: !expand "{doc.places.PROJECT_DIR}/{tools.env('USER')}/save"
